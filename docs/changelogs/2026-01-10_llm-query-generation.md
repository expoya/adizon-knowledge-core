# LLM-basierte Query-Generierung & Robustes JSON Parsing

**Datum:** 2026-01-10  
**Typ:** üîß Bug Fix + ‚ú® Feature Enhancement  
**Betrifft:** `query_service.py`, `metadata_store.py`, `chat_workflow.py`

## Problem

### 1. **Fragiles Keyword-Parsing mit Regex crasht bei Sonderzeichen**

```python
# VORHER (‚ùå Crasht bei #, @, etc.)
def _extract_keywords(self, question: str) -> List[str]:
    words = re.findall(r'\b[A-Z][a-z√§√∂√º√ü]+(?:\s+[A-Z][a-z√§√∂√º√ü]+)*\b', question)
    lowercase_words = re.findall(r'\b[a-z√§√∂√º√ü]{4,}\b', question.lower())
    stopwords = {"what", "when", "where", ...}
    keywords = [w for w in lowercase_words if w not in stopwords]
    return list(set(words + keywords))
```

**Problem:**
- Komplexe Regex-Patterns
- Stopword-Listen (unvollst√§ndig)
- Crasht bei Sonderzeichen: `#`, `@`, Emojis, etc.
- Kein semantisches Verst√§ndnis

### 2. **JSON Parsing crasht bei Control Characters**

```
ERROR: Failed to parse LLM response: Invalid control character at: line 2 column 17
```

**Problem:** LLM gibt manchmal JSON mit `\n`, `\t`, `\r` zur√ºck  
**Betroffen:** `metadata_store.py`, `chat_workflow.py`, `query_service.py`

### 3. **Score-Bug in Entity Resolution**

```python
best_score = best_match.get("total_score", 0)  # ‚ùå Query gibt "score" zur√ºck!
```

Query returned `score`, Code suchte nach `total_score` ‚Üí Score war immer 0!

## L√∂sung

### 1. LLM-basierte Query-Generierung (ü§ñ Robust & Semantisch!)

**Neues Prompt:** `query_generation.txt`

```
Du bist ein Query-Generator f√ºr eine Wissensdatenbank.

Extrahiere aus der User-Anfrage die WICHTIGSTEN SUCHBEGRIFFE.

Regeln:
1. Namen von Personen, Firmen, Produkten
2. Wichtige Substantive (keine F√ºllw√∂rter)
3. 2-5 Suchbegriffe
4. Behalte Original-Schreibweise

User-Anfrage: {query}

Output: ["Begriff 1", "Begriff 2", ...]
```

**Beispiele:**
```
User: "Welche Notizen haben wir zu Samuel Wolf?"
LLM: ["Samuel Wolf", "Notizen"]

User: "Zeig mir alle Rechnungen von Lumix Solutions GmbH"
LLM: ["Lumix Solutions GmbH", "Rechnungen"]

User: "Was kostet das #Premium Paket?"
LLM: ["Premium Paket", "Preis"]  # ‚úÖ Keine Probleme mit #
```

### 2. Robustes JSON Parsing (Control Character Cleaning)

```python
# NACHHER (‚úÖ Robust)
import re
content = result.content.strip()

# Remove markdown code blocks
if content.startswith("```"):
    content = content.split("```")[1]
    if content.startswith("json"):
        content = content[4:]
content = content.strip()

# Clean control characters that break JSON parsing
content = re.sub(r'[\x00-\x1F\x7F]', ' ', content)

keywords = json.loads(content)  # ‚úÖ Funktioniert jetzt!
```

**Angewendet auf:**
- `query_service.py` ‚Üí `_extract_keywords()`
- `metadata_store.py` ‚Üí `get_relevant_sources_llm()`
- `chat_workflow.py` ‚Üí Entity Extraction (2x)

### 3. Score-Bug Fix

```python
# VORHER (‚ùå)
best_score = best_match.get("total_score", 0)

# NACHHER (‚úÖ)
best_score = best_match.get("score", 0)
```

## Ge√§nderte Dateien

### 1. `backend/app/prompts/query_generation.txt` (NEU)
- LLM Prompt f√ºr Query-Generierung
- Extrahiert 2-5 relevante Suchbegriffe
- JSON-Array Output

### 2. `backend/app/services/graph_operations/query_service.py`

**`_extract_keywords()` (Zeilen 192-256):**
```python
async def _extract_keywords(self, question: str) -> List[str]:
    """LLM-basierte Keyword-Extraktion (robust gegen Sonderzeichen)."""
    try:
        llm = get_llm(temperature=0.0, streaming=False)
        query_prompt = get_prompt("query_generation")
        
        result = await llm.ainvoke([
            SystemMessage(content=query_prompt.format(query=question))
        ])
        
        # Parse JSON with control char cleaning
        content = result.content.strip()
        # ... markdown removal ...
        content = re.sub(r'[\x00-\x1F\x7F]', ' ', content)
        
        keywords = json.loads(content)
        return keywords
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è LLM keyword extraction failed: {e}")
        return self._fallback_keywords(question)
```

**Fallback f√ºr Robustheit:**
```python
def _fallback_keywords(self, question: str) -> List[str]:
    """Einfacher Fallback: Extrahiere kapitalisierte W√∂rter (Namen)."""
    words = re.findall(r'\b[A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+(?:\s+[A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+)*\b', question)
    return list(set(words)) if words else [""]
```

### 3. `backend/app/services/metadata_store.py`

**`get_relevant_sources_llm()` (Zeile ~347):**
```python
# Clean control characters before JSON parsing
content = re.sub(r'[\x00-\x1F\x7F]', ' ', content)
result = json.loads(content)
```

### 4. `backend/app/graph/chat_workflow.py`

**Score-Bug Fix (Zeilen ~398, ~405):**
```python
best_score = best_match.get("score", 0)  # FIXED: score statt total_score
# ...
score = entity.get("score", 0)  # FIXED: score statt total_score
```

**Entity Extraction - Control Char Cleaning (2x):**
```python
# Router Node (Zeile ~110)
extracted_text = re.sub(r'[\x00-\x1F\x7F]', ' ', extracted_text)
entity_names = json.loads(extracted_text)

# Knowledge Node (Zeile ~318)
extracted_text = re.sub(r'[\x00-\x1F\x7F]', ' ', extracted_text)
entity_names = json.loads(extracted_text)
```

## Erwartetes Verhalten

### Vorher (‚ùå)

```
User: "Welche Notizen haben wir zu Samuel Wolf? #urgent"
System: [Regex crasht wegen #]
ERROR: Invalid pattern

User: "Lumix Solutions GmbH - Rechnungen"
LLM Response: {"reasoning": "Check...\n..."}
ERROR: Invalid control character at: line 2 column 17
```

### Nachher (‚úÖ)

```
User: "Welche Notizen haben wir zu Samuel Wolf? #urgent"

Log:
  ü§ñ LLM extracting search keywords...
  ‚úÖ LLM extracted keywords: ["Samuel Wolf", "Notizen"]
  
Graph Query:
  MATCH (n) WHERE ... CONTAINS "Samuel Wolf" OR ... CONTAINS "Notizen"
  
Response: [Zeigt alle Notizen zu Samuel Wolf]
```

```
User: "Lumix Solutions GmbH - Rechnungen"

Log:
  ü§ñ LLM extracting search keywords...
  ‚úÖ LLM extracted keywords: ["Lumix Solutions GmbH", "Rechnungen"]
  
Response: [Zeigt alle Rechnungen von Lumix Solutions GmbH]
```

## Vorteile

### ü§ñ LLM Query-Generierung

1. **Robust gegen Sonderzeichen:** `#`, `@`, Emojis, etc. ‚Üí kein Problem
2. **Semantisches Verst√§ndnis:** "Zahlungsstatus" ‚Üí ["Rechnungen", "Status"]
3. **Keine Stopword-Listen:** LLM erkennt F√ºllw√∂rter selbst
4. **Multi-Language:** Funktioniert mit DE/EN/gemischt
5. **Kontextabh√§ngig:** "Premium Paket" wird als EIN Begriff erkannt

### üõ°Ô∏è Robustes JSON Parsing

1. **Control Character Cleaning:** `\n`, `\t`, `\r` werden zu Spaces
2. **Markdown-Removal:** Extrahiert JSON aus ```json ... ``` Bl√∂cken
3. **Graceful Degradation:** Bei Fehler ‚Üí Fallback
4. **Konsistent √ºberall:** Gleicher Code in allen 3 Dateien

## Testing

**Manuelle Tests:**

1. ‚úÖ "Welche Notizen haben wir zu Samuel Wolf #urgent?"
2. ‚úÖ "Zeig mir Rechnungen von @Lumix Solutions GmbH"
3. ‚úÖ "Was kostet das Premium Paket? üí∞"
4. ‚úÖ "Lumix Solutions GmbH - Status?"
5. ‚úÖ "Wie viele Leads haben wir insgesamt?"

**Score-Bug Test:**
```
User: "Welche Notizen haben wir zu Samuel Wolf?"

Log VORHER:
  ‚úÖ Best match: Contact 'Samuel Wolf' (Score: 100)
  ‚ö†Ô∏è Low confidence match (Score: 0)  # ‚ùå Bug!

Log NACHHER:
  ‚úÖ Best match: Contact 'Samuel Wolf' (Score: 100)
  üéØ Confident match (Score: 100)  # ‚úÖ Korrekt!
```

## Migration / Rollout

‚úÖ **Keine Breaking Changes**
- Backwards-kompatibel
- Fallback bei LLM-Fehler
- Keine API-√Ñnderungen
- Keine DB-√Ñnderungen

**Empfohlene Schritte:**
1. Deploy Code
2. Monitoring auf LLM Query-Generierung Logs
3. Testen mit bekannten problematischen Queries

## Performance

**LLM Query-Generierung:**
- +1 LLM Call pro Graph Query (~200-500ms)
- Acceptable f√ºr bessere Robustheit & Genauigkeit
- Caching m√∂glich (future improvement)

**JSON Parsing:**
- Regex `re.sub()` ist sehr schnell (~microseconds)
- Vernachl√§ssigbarer Overhead

## Hinweise

- **Fallback ist robust:** Bei LLM-Fehler ‚Üí Regex-Fallback (nur Namen)
- **Control Char Cleaning ist safe:** Ersetzt nur unsichtbare Zeichen
- **Score-Bug war kritisch:** F√ºhrte zu falschen "Low Confidence" Warnings

## Siehe auch

- [2026-01-10_llm-entity-extraction.md](./2026-01-10_llm-entity-extraction.md) - Entity Extraction mit LLM
- [2026-01-10_graph-query-incoming-relations-fix.md](./2026-01-10_graph-query-incoming-relations-fix.md) - Incoming Relations Fix
- [AGENTIC_RAG.md](../AGENTIC_RAG.md) - RAG Architektur

---

**Status:** ‚úÖ Implementiert  
**Autor:** Michael Schiestl  
**Review:** Pending

